{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e441bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Before running this code, you will need to:\n",
    "# (1) Go to https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TransitView/nph-visibletbls?dataset=transits\n",
    "# (2) Upload the target file (if multiple) or enter the planet name (if single). \n",
    "#     TOIs/TIC planets may need to be entered by hand if their coordinates aren't known\n",
    "# (3) Select Earth Observatory. Coordinates in deg:\n",
    "#     Challis: N lat =44.49833 E long = -114.33388 (alt = 2165.0 ) (4) Set custom start/end date (note in UT)\n",
    "#     IoIO? \n",
    "#     SAAO: select option\n",
    "# (4) Currently working with a file that has been pre-filtered for observability, but we'll add that here.\n",
    "\n",
    "#       SAAO 1-m: above 30 deg altitude during entire window\n",
    "#       SAAO 74 in (Lasedi): complicated function\n",
    "#       Challis:\n",
    "#       IoIO:\n",
    "# (5) To do: I already removed the duplicates by hand (several different ephemerides, for some planets)\n",
    "#            I filter for transits that have 1+ hours of baseline above 30 deg alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31b50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# pip install astropy\n",
    "# conda install astropy\n",
    "from astropy.time import Time\n",
    "from astropy.table import Table\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080d4eb",
   "metadata": {},
   "source": [
    "# When are we predicting for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "261dc297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions are for this observatory: Challis\n",
      "Using ephemeris file ephem_files/Challis-transits_2022.04.12_08.57.09.csv\n",
      "Which exists? True\n"
     ]
    }
   ],
   "source": [
    "### CHANGE: when = pick a name for when you are running ; also set the file name\n",
    "##################\n",
    "when = \"Challis-transits-Apr-May-2022\" # the first part should be \"IoIO\", \"Challis\", or \"SAAO\"\n",
    "##############\n",
    "use_obs = when.replace(\"_\",\"-\").split(\"-\")[0]\n",
    "print(\"Predictions are for this observatory:\",use_obs)\n",
    "\n",
    "file_for_when={}\n",
    "\n",
    "### CHANGE: add a new line for each when to set the file name\n",
    "file_for_when[\"IoIO_TOI-2109_Spring_2022\"]     = \"ephem_files/TOI-2109b_transits_2022.03.09_08.42.22.csv\"\n",
    "file_for_when[\"SAAO_summer_2022\"]              = \"ephem_files/SAAO_2022.05-08_all_possible.csv\"\n",
    "file_for_when[\"Challis-transits-Apr-May-2022\"] = \"ephem_files/Challis-transits_2022.04.12_08.57.09.csv\"\n",
    "\n",
    "### No need to change anything below\n",
    "if when in file_for_when.keys():\n",
    "    ephem_file = file_for_when[when]\n",
    "    print(\"Using ephemeris file\",ephem_file)\n",
    "    print(\"Which exists?\",os.path.isfile(ephem_file))\n",
    "else:\n",
    "    print(\"File not defined for\",when)\n",
    "      \n",
    "if use_obs == \"IoIO\":\n",
    "    dusk = \"22\" \n",
    "    dawn = \"16\"\n",
    "elif use_obs == \"SAAO\":\n",
    "    # SAAO is UT+2; these start/stop times are UT, and not exact\n",
    "    dusk = \"13\"\n",
    "    dawn = \"08\"\n",
    "elif use_obs == \"Challis\":\n",
    "    dusk = \"22\" \n",
    "    dawn = \"16\"\n",
    "else:\n",
    "    print(\"STOP! We need a valid observatory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6565910",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = when +\"/\"\n",
    "if os.path.isdir(out_folder)==False:\n",
    "    os.makedirs(out_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e682a",
   "metadata": {},
   "source": [
    "# Other info on the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0cdbfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Planet', 'V Mag', 'Dec', 'Period (days)', 'Period (days) err', 'Depth (%)', 'R_P R_E', 'R_p_err_plus', 'R_p_err_minus', 'disc_year', 'Err (min/5 yr)', 'Tau_p', 'Years since discovery']\n"
     ]
    }
   ],
   "source": [
    "### CHANGE  directory\n",
    "xrp_target_info_file=\"XRP_single_table-Combined.csv\"\n",
    "target_data = Table.read(xrp_target_info_file, format='ascii.csv', comment='#')\n",
    "print(target_data.colnames)\n",
    "tar_planets = list(target_data[\"Planet\"])\n",
    "tar_depth = list(target_data[\"Depth (%)\"])\n",
    "tar_err_5yr = list(target_data[\"Err (min/5 yr)\"])\n",
    "tar_tau =  list(target_data[\"Tau_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dc74978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TrES-3 b': 2.7, 'WASP-4 b': 2.2, 'WASP-3 b': 1.0, 'WASP-12 b': 1.0, 'WASP-5 b': 1.2, 'CoRoT-2 b': 2.6, 'WASP-19 b': 2.0, 'WASP-18 b': 0.9, 'HAT-P-23 b': 1.3, 'WASP-33 b': 1.1, 'Qatar-1 b': 2.1, 'WASP-43 b': 2.5, 'HAT-P-36 b': 1.4, 'WASP-36 b': 1.9, 'WASP-50 b': 2.0, 'WASP-77 A b': 1.7, 'WASP-64 b': 1.5, 'WASP-52 b': 2.5, 'WASP-103 b': 1.2, 'WASP-104 b': 1.5, 'K2-31 b': 2.0, 'WASP-121 b': 1.4, 'WASP-114 b': 0.9, 'KELT-14 b': 1.3, 'WASP-76 b': 1.2, 'HATS-35 b': 1.1, 'KELT-16 b': 1.1, 'HATS-24 b': 1.6, 'WASP-173 A b': 1.2, 'WASP-163 b': 1.5, 'KPS-1 b': 1.4, 'WASP-145 A b': 1.9, 'WASP-164 b': 1.6, 'Qatar-10 b': 1.6, 'TIC 427761355': 0.9, 'HIP 65 A b': 1.5, 'TIC 468574941': 1.7, 'TOI-564 b': 0.9}\n"
     ]
    }
   ],
   "source": [
    "tar_depth_hash = {}\n",
    "for ii,pp in enumerate(tar_planets):\n",
    "    tar_depth_hash[pp] = tar_depth[ii]\n",
    "print(tar_depth_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743460e",
   "metadata": {},
   "source": [
    "# Read in ephemeris file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8668ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['planetname', 'ra_str', 'ra', 'dec_str', 'dec', 'obsname', 'reflink', 'algorithm', 'isdefault', 'ismostprecise', 'hasmoreprecise', 'eccenwarning', 'phase', 'period', 'transitduration', 'transitdepthdb', 'transitdepthcalc', 'ttv', 'ingresscalendar', 'ingressjd', 'ingressaltitude', 'midpointcalendar', 'propmidpointunc', 'midpointairmass', 'midpointaltitude', 'egresscalendar', 'egressjd', 'egressaltitude', 'targetobsstartcalendar', 'targetobsendcalendar', 'durationbeforeevent', 'durationafterevent', 'fractionobservable', 'magnitude_visible']\n"
     ]
    }
   ],
   "source": [
    "ephem_data = Table.read(ephem_file, format='ascii.csv', comment='#')\n",
    "print(ephem_data.colnames)\n",
    "planet_name_with_dupes = list(ephem_data[\"planetname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f65ec8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TrES-3 b', 'WASP-4 b', 'WASP-3 b', 'WASP-12 b', 'WASP-5 b', 'CoRoT-2 b', 'WASP-19 b', 'WASP-18 b', 'HAT-P-23 b', 'WASP-33 b', 'Qatar-1 b', 'WASP-43 b', 'HAT-P-36 b', 'WASP-36 b', 'WASP-50 b', 'WASP-77 A b', 'WASP-64 b', 'WASP-52 b', 'WASP-103 b', 'WASP-104 b', 'K2-31 b', 'WASP-121 b', 'WASP-114 b', 'KELT-14 b', 'WASP-76 b', 'HATS-35 b', 'KELT-16 b', 'HATS-24 b', 'WASP-173 A b', 'WASP-163 b', 'KPS-1 b', 'WASP-145 A b', 'WASP-164 b', 'Qatar-10 b', 'TIC 427761355', 'HIP 65 A b', 'TIC 468574941', 'TOI-564 b'])\n"
     ]
    }
   ],
   "source": [
    "# If some planets aren't in this list...\n",
    "all_planets = list(set(planet_name_with_dupes))\n",
    "print(tar_depth_hash.keys())\n",
    "for pp in all_planets:\n",
    "    if pp not in tar_depth_hash.keys():\n",
    "        tar_depth_hash[pp] = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5583a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1592 2.7883\n"
     ]
    }
   ],
   "source": [
    "ephem_durations_with_dupes = list(ephem_data['transitduration'])\n",
    "print(min(ephem_durations_with_dupes),max(ephem_durations_with_dupes))\n",
    "ephem_alt_in_with_dupes = list(ephem_data['ingressaltitude'])\n",
    "ephem_alt_eg_with_dupes = list(ephem_data['egressaltitude'])\n",
    "ephem_alt_mid_with_dupes = list(ephem_data['midpointairmass'])\n",
    "## Get the calendar times for ingress and egress and padded\n",
    "ephem_in_cal_with_dupes = list(ephem_data['ingresscalendar'])\n",
    "ephem_eg_cal_with_dupes = list(ephem_data['egresscalendar'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d346019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephem_midtimes_with_dupes = list(ephem_data[\"midpointcalendar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "982978df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/22/2022 09:40\n"
     ]
    }
   ],
   "source": [
    "print(ephem_midtimes_with_dupes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "962756a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eph_to_utc_time(tt):\n",
    "    parts = tt.split(\" \")\n",
    "    dmy = parts[0]\n",
    "    time_ut = parts[1]\n",
    "    date = dmy.split(\"/\")\n",
    "    reformatted = date[2]+\"-\"+date[0]+\"-\"+date[1]+\"T\"+time_ut+\":00\"\n",
    "    return reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d18caa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephem_midtimes_utc_with_dupes = [eph_to_utc_time(aa) for aa in ephem_midtimes_with_dupes]\n",
    "ephem_in_utc_with_dupes = [eph_to_utc_time(aa) for aa in ephem_in_cal_with_dupes]\n",
    "ephem_eg_utc_with_dupes = [eph_to_utc_time(aa) for aa in ephem_eg_cal_with_dupes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba8848b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephem_Times_with_dupes = Time(ephem_midtimes_utc_with_dupes, format='isot')\n",
    "ephem_in_Times_with_dupes = Time(ephem_in_utc_with_dupes, format='isot')\n",
    "ephem_eg_Times_with_dupes = Time(ephem_eg_utc_with_dupes, format='isot')\n",
    "ephem_midtimes_jd_with_dupes = ephem_Times_with_dupes.jd\n",
    "ephem_in_jd_with_dupes = ephem_in_Times_with_dupes.jd\n",
    "ephem_eg_jd_with_dupes = ephem_eg_Times_with_dupes.jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a80cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-22T09:40:00 2459691.902777778\n"
     ]
    }
   ],
   "source": [
    "print(ephem_midtimes_utc_with_dupes[0],ephem_Times_with_dupes[0].jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0dfeb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2459691.85625 2459691.949305556\n"
     ]
    }
   ],
   "source": [
    "print(ephem_in_jd_with_dupes[0], ephem_eg_jd_with_dupes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b392977",
   "metadata": {},
   "source": [
    "## Get rid of duplicate transits early so it doesn't mess up the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2289d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "0 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "1 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "2 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "3 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "4 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "5 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "6 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "7 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "8 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "9 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "10 [60]\n",
      "duplicate! 60 sec HAT-P-36 b HAT-P-36 b\n",
      "11 [60]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1980 sec Qatar-1 b Qatar-1 b\n",
      "12 [240, 0, 240, 1980]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "13 [240, 0, 240, 1920]\n",
      "duplicate! 180 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "14 [180, 0, 240]\n",
      "15 []\n",
      "16 []\n",
      "17 []\n",
      "18 []\n",
      "19 []\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2220 sec Qatar-1 b Qatar-1 b\n",
      "20 [240, 240, 0, 2220]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "21 [240, 240, 0, 2160]\n",
      "duplicate! 180 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 180 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 60 sec Qatar-1 b Qatar-1 b\n",
      "22 [180, 180, 60]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "23 [240, 0, 240, 2160]\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1980 sec Qatar-1 b Qatar-1 b\n",
      "24 [0, 240, 240, 1980]\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "25 [0, 240, 240, 1920]\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 180 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "26 [0, 180, 240]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "27 [240, 240, 0, 1920]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2220 sec Qatar-1 b Qatar-1 b\n",
      "28 [240, 0, 240, 2220]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "29 [240, 0, 240, 2160]\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 60 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "30 [240, 60, 240]\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "31 [0, 240, 240, 2160]\n",
      "32 []\n",
      "33 []\n",
      "34 []\n",
      "35 []\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 0 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 240 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "36 [240, 0, 240, 1920]\n",
      "duplicate! 1980 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2220 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1980 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2220 sec Qatar-1 b Qatar-1 b\n",
      "37 [1980, 2220, 1980, 2220]\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "38 [1920, 2160, 1920, 2160]\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 2160 sec Qatar-1 b Qatar-1 b\n",
      "duplicate! 1920 sec Qatar-1 b Qatar-1 b\n",
      "39 [2160, 1920, 2160, 1920]\n",
      "40 []\n",
      "41 []\n",
      "42 []\n",
      "43 []\n",
      "44 []\n",
      "duplicate! 240 sec WASP-3 b WASP-3 b\n",
      "duplicate! 180 sec WASP-3 b WASP-3 b\n",
      "duplicate! 600 sec WASP-3 b WASP-3 b\n",
      "45 [240, 180, 600]\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "duplicate! 840 sec WASP-3 b WASP-3 b\n",
      "duplicate! 240 sec WASP-3 b WASP-3 b\n",
      "46 [420, 840, 240]\n",
      "duplicate! 240 sec WASP-3 b WASP-3 b\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "duplicate! 840 sec WASP-3 b WASP-3 b\n",
      "47 [240, 420, 840]\n",
      "48 []\n",
      "49 []\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "duplicate! 180 sec WASP-3 b WASP-3 b\n",
      "50 [420, 420, 180]\n",
      "duplicate! 180 sec WASP-3 b WASP-3 b\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "51 [180, 420, 420]\n",
      "duplicate! 840 sec WASP-3 b WASP-3 b\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "duplicate! 600 sec WASP-3 b WASP-3 b\n",
      "52 [840, 420, 600]\n",
      "duplicate! 600 sec WASP-3 b WASP-3 b\n",
      "duplicate! 840 sec WASP-3 b WASP-3 b\n",
      "duplicate! 420 sec WASP-3 b WASP-3 b\n",
      "53 [600, 840, 420]\n",
      "duplicate! 240 sec WASP-3 b WASP-3 b\n",
      "duplicate! 180 sec WASP-3 b WASP-3 b\n",
      "duplicate! 600 sec WASP-3 b WASP-3 b\n",
      "54 [240, 180, 600]\n",
      "55 []\n"
     ]
    }
   ],
   "source": [
    "# Assume that any transit of the same planet that is within 1 hour is a duplicate \n",
    "# (shortest possible periods are P=4 hr)\n",
    "has_alt_ephem={}\n",
    "for ii in range(len(planet_name_with_dupes)):\n",
    "    dupe_diff=[]; dupe_indices1=[]\n",
    "    mt1 = ephem_midtimes_jd_with_dupes[ii]\n",
    "    for jj in range(len(planet_name_with_dupes)):\n",
    "        if ii != jj:\n",
    "            mt2 = ephem_midtimes_jd_with_dupes[jj]\n",
    "            diff = abs(mt1-mt2)\n",
    "            if diff < 1/24.: # 1 hour\n",
    "                #print(round(diff*86400),\"sec\",planet_name[ii],planet_name[jj])\n",
    "                if planet_name_with_dupes[ii] == planet_name_with_dupes[jj]:\n",
    "                    print(\"duplicate!\",round(diff*86400),\"sec\",\n",
    "                          planet_name_with_dupes[ii],\n",
    "                          planet_name_with_dupes[jj])\n",
    "                    dupe_diff.append(round(diff*86400))\n",
    "                    dupe_indices1.append([ii,jj])\n",
    "    dupe_indices = sorted([item for sublist in dupe_indices1 for item in sublist])\n",
    "\n",
    "    # We want to flag these duplicates since they have uncertain ephemerides\n",
    "    print(ii, dupe_diff)\n",
    "    if dupe_diff != []:\n",
    "        has_alt_ephem[ii] = dupe_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe48fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 29\n",
      "[0, 1, 2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 18, 19, 23, 32, 33, 34, 35, 40, 41, 42, 43, 44, 45, 46, 48, 49, 55]\n"
     ]
    }
   ],
   "source": [
    "## Now make the lists\n",
    "using_ind=[]\n",
    "for ii in range(len(planet_name_with_dupes)):\n",
    "    if ii in has_alt_ephem.keys():\n",
    "       # print(ii,has_alt_ephem[ii])\n",
    "        # arbitrarily pick the FIRST prediction (rather than weighting/picking \"best\")\n",
    "        use = has_alt_ephem[ii][0]\n",
    "        if use not in using_ind:\n",
    "            using_ind.append(use)\n",
    "    else:\n",
    "        using_ind.append(ii)\n",
    "print(len(planet_name_with_dupes),len(using_ind))\n",
    "print(using_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6c2ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "#### USE these non-duplicated lists\n",
    "planet_name = [planet_name_with_dupes[ii] for ii in using_ind]\n",
    "ephem_durations = [ephem_durations_with_dupes[ii] for ii in using_ind]\n",
    "ephem_alt_in = [ephem_alt_in_with_dupes[ii] for ii in using_ind]\n",
    "ephem_alt_eg = [ephem_alt_eg_with_dupes[ii] for ii in using_ind]\n",
    "ephem_alt_mid = [ephem_alt_mid_with_dupes[ii] for ii in using_ind]\n",
    "ephem_midtimes = [ephem_midtimes_with_dupes[ii] for ii in using_ind]\n",
    "ephem_midtimes_utc = [ephem_midtimes_utc_with_dupes[ii] for ii in using_ind]\n",
    "ephem_Times = [ephem_Times_with_dupes[ii] for ii in using_ind]\n",
    "ephem_midtimes_jd = [ephem_midtimes_jd_with_dupes[ii] for ii in using_ind]\n",
    "print(len(ephem_durations))\n",
    "\n",
    "planet_ra = [list(ephem_data['ra_str'])[ii] for ii in using_ind]\n",
    "planet_dec = [list(ephem_data['dec_str'])[ii] for ii in using_ind]\n",
    "planet_vmag = [list(ephem_data['magnitude_visible'])[ii] for ii in using_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4e63a",
   "metadata": {},
   "source": [
    "## What times are the actual transits, with and without padding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "683a97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you want to have shorter baseline, you can decrease this number but not below about 0.5\n",
    "pad_by = 1. # hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ce8fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingress_jd = []; egress_jd = []; padded_start_jd = []; padded_end_jd = []\n",
    "for ii,tt in enumerate(ephem_Times):\n",
    "    dur = ephem_durations[ii]\n",
    "    ingress_jd.append( tt.jd - dur/2./24. )\n",
    "    egress_jd.append( tt.jd + dur/2./24. )\n",
    "    padded_start_jd.append( tt.jd - dur/2./24. - pad_by/24.)\n",
    "    padded_end_jd.append( tt.jd + dur/2./24. + pad_by/24.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a118da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingress 2459691.8562673614\n",
      "Egress 2459691.9492881945\n",
      "Padded Ingress 2459691.814600695\n",
      "Padded Egress 2459691.990954861\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(\"Ingress\",ingress_jd[0])\n",
    "print(\"Egress\",egress_jd[0])\n",
    "print(\"Padded Ingress\",padded_start_jd[0])\n",
    "print(\"Padded Egress\",padded_end_jd[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31435c",
   "metadata": {},
   "source": [
    "## Which transits are available on which nights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "476e15cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14\n",
      "2022-05-13\n"
     ]
    }
   ],
   "source": [
    "### Check: do you want the first day to be the first day listed in the file\n",
    "use_first_ephem_day = True # only pick false if trying to match another file\n",
    "if use_first_ephem_day:\n",
    "    ff = min(ephem_in_cal_with_dupes)\n",
    "    dd = ff.split(\" \")[0].split(\"/\")\n",
    "    first_day = dd[2]+\"-\"+dd[0]+\"-\"+dd[1]\n",
    "    print(first_day)\n",
    "    ll = max(ephem_eg_cal_with_dupes)\n",
    "    dd = ll.split(\" \")[0].split(\"/\")\n",
    "    last_day = dd[2]+\"-\"+dd[0]+\"-\"+dd[1]\n",
    "    print(last_day)\n",
    "### XX check later: this might run into problems with cutting off the first date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4f1d4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_fiducial_night = (24 - int(dusk) + int(dawn))/24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f14efd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2459684.4166666665 2022-04-14T22:00:00.000\n",
      "2459685.1666666665 2022-04-15T16:00:00.000\n"
     ]
    }
   ],
   "source": [
    "first_day_starts = Time(first_day + \"T\"+dusk+\":00:00\", format='isot')\n",
    "last_day_starts = Time(last_day + \"T\"+dusk+\":00:00\", format='isot')\n",
    "print(first_day_starts.jd, first_day_starts)\n",
    "first_day_ends = first_day_starts + length_fiducial_night\n",
    "last_day_ends = last_day_starts + length_fiducial_night\n",
    "print(first_day_ends.jd, first_day_ends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27bf3c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0\n"
     ]
    }
   ],
   "source": [
    "n_days = last_day_starts.jd-first_day_starts.jd\n",
    "print(n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "609e7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_start=[]; days_end =[]\n",
    "for ii in range(int(n_days)+1):\n",
    "    t = first_day_starts.jd + ii\n",
    "    days_start.append(t )\n",
    "    days_end.append( first_day_ends.jd+ii )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d44f031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_start_utc = Time(days_start,format=\"jd\").isot\n",
    "date_start_utc = [days_start_utc[xx].split(\"T\")[0] for xx in range(len(days_start_utc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77768892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2459684.4166666665 2459685.1666666665\n"
     ]
    }
   ],
   "source": [
    "print(first_day_starts.jd, first_day_ends.jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0ba9050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPS-1 b 04/14/2022 09:23 Not in the time range\n"
     ]
    }
   ],
   "source": [
    "# Find which day each transit belongs on\n",
    "transit_on_day=[]\n",
    "for ii,tt in enumerate(padded_start_jd):\n",
    "    found=False\n",
    "    #print(\"\\n\",planet_name[ii], \"start\",padded_start_jd[ii],\"mid\",ephem_midtimes[ii],ephem_Times[ii].jd)\n",
    "    for dd in range(len(days_start)):\n",
    "        if (tt > days_start[dd]):\n",
    "            if tt < days_end[dd]:\n",
    "                transit_on_day.append(dd)\n",
    "                found=True\n",
    "    if found==False:\n",
    "        transit_on_day.append(None)\n",
    "        print(planet_name[ii],ephem_midtimes[ii],\"Not in the time range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db49b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 11, 15, 19, 23, 27, 7, 17, 24, None, 6, 11, 23, 28, 27, 4, 8, 21, 25, 6, 11, 3, 16, 28, 18, 5, 4, 11, 1]\n",
      "29\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(transit_on_day)\n",
    "print(len(transit_on_day))\n",
    "print(len(ephem_midtimes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aca085",
   "metadata": {},
   "source": [
    "## Now figure out which days are the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7607d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transits_per_day=[]\n",
    "for dd in range(len(days_start)):\n",
    "    transits_per_day.append(transit_on_day.count(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "59701171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days with  0  transit(s) 11\n",
      "Days with  1  transit(s) 12\n",
      "Days with  2  transit(s) 6\n",
      "Days with  3  transit(s) 0\n",
      "Days with  4  transit(s) 1\n",
      "Days with  5  transit(s) 0\n",
      "Days with  6  transit(s) 0\n",
      "Days with  7  transit(s) 0\n",
      "Days with  8  transit(s) 0\n",
      "Days with  9  transit(s) 0\n"
     ]
    }
   ],
   "source": [
    "### Look here\n",
    "for ii in range(10):\n",
    "    print(\"Days with \",ii,\" transit(s)\", transits_per_day.count(ii))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185afb66",
   "metadata": {},
   "source": [
    "### Of the days with 2+ transits, how many have more than one non-overlapping transits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "691b20fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(transit_on_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "465c7cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.04999999999999999\n",
      "0.06\n"
     ]
    }
   ],
   "source": [
    "def do_transits_overlap(start1,end1,start2,end2):\n",
    "    if (start1>= end2) | (start2>=end1):\n",
    "        return 0\n",
    "    else:\n",
    "        if start1 < start2: # happens first\n",
    "            return end1 - start2\n",
    "        elif start2 < start1:\n",
    "            return end2 - start1\n",
    "# test\n",
    "print(do_transits_overlap(0.1,0.2, 0.3, 0.4)) # no\n",
    "print(do_transits_overlap(0.1,0.35, 0.3, 0.4)) # yes\n",
    "print(do_transits_overlap(0.4,0.5, 0.3, 0.46)) # no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d40e57",
   "metadata": {},
   "source": [
    "### Information on each transit happening by day number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9b7ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_transit_hash = {}\n",
    "for ii,tt in enumerate(transits_per_day):\n",
    "    all_daily_transits = []\n",
    "    for jj,day in enumerate(transit_on_day):\n",
    "        if day != None:\n",
    "            if ii==day:\n",
    "                all_daily_transits.append([planet_name[jj],padded_start_jd[jj],padded_end_jd[jj]])\n",
    "    daily_transit_hash[ii] = all_daily_transits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e1fea2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0, 4, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 0, 2, 2, 0]\n",
      "30\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "daily_transit_hash[11]\n",
    "print(transits_per_day)\n",
    "print(len(transits_per_day))\n",
    "print(len(date_start_utc))\n",
    "print(len(daily_transit_hash.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099569b2",
   "metadata": {},
   "source": [
    "### Get rid of overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816e359",
   "metadata": {},
   "source": [
    "We can't observe two transits at the same time, so we need to get rid of the ones that are completely overlapping. This will also weed out duplicates from the ephemeris program, which gives several predictions for the same transit for some systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a1ad326d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020833333333333332"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How much overlap is okay?\n",
    "okay_overlap = 0./60 /24. # 0 minutes\n",
    "30./60/24 # 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "460eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overlap=[]; \n",
    "for ii,tt in enumerate(transits_per_day):\n",
    "    if tt >1:\n",
    "        all_daily_transits = []\n",
    "        for jj,day in enumerate(transit_on_day):\n",
    "            if day != None:\n",
    "                if day==ii:\n",
    "                    #print(planet_name[jj],\"start\",padded_start_jd[jj],\"end\",padded_end_jd[jj])\n",
    "                    all_daily_transits.append([planet_name[jj],padded_start_jd[jj],padded_end_jd[jj]])\n",
    "        # Now check if transits are overlapping\n",
    "        for xx in range(len(all_daily_transits)):\n",
    "            for yy in range(len(all_daily_transits)):\n",
    "                if xx != yy:\n",
    "                    if all_daily_transits[xx][0] == all_daily_transits[yy][0]:\n",
    "                        print(\"Duplicate! Highly unlikely two transits of same planet on same night\", all_daily_transits[xx][0])\n",
    "                    else:\n",
    "                         # aim for zero overlap unless pressed (set above)\n",
    "                        overlap = do_transits_overlap(all_daily_transits[xx][1],all_daily_transits[xx][2],\n",
    "                                             all_daily_transits[yy][1],all_daily_transits[yy][2])\n",
    "                        if overlap == None:\n",
    "                            overlap = 0.\n",
    "                        if overlap <= okay_overlap:\n",
    "                            info = [ii, days_start[ii], date_start_utc[ii], sorted([all_daily_transits[xx][0], all_daily_transits[yy][0]])]\n",
    "                            if info not in no_overlap:\n",
    "                                no_overlap.append(info)\n",
    "#                            print(ii, days_start[ii], all_daily_transits[xx][0], all_daily_transits[yy][0],\"no overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e7c174db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796e611",
   "metadata": {},
   "source": [
    "## Now pick observing chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c9170cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overlap_day_nums = [no_overlap[xx][0] for xx in range(len(no_overlap))]\n",
    "no_overlap_dates = [no_overlap[xx][2] for xx in range(len(no_overlap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "09eeefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_of_two=[] # number of consecutive days\n",
    "groups_of_three=[] # number of consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12bb3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nn in no_overlap_day_nums:\n",
    "    if nn + 1 in no_overlap_day_nums:\n",
    "        if nn+2 in no_overlap_day_nums:\n",
    "            groups_of_three.append([nn,nn+1,nn+2])\n",
    "for nn in no_overlap_day_nums:\n",
    "    if nn + 1 in no_overlap_day_nums:\n",
    "        groups_of_two.append([nn,nn+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "97470075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(groups_of_three)\n",
    "print(groups_of_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9433a2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two nights in a row, starting\n",
      "Three nights in a row, starting\n"
     ]
    }
   ],
   "source": [
    "print(\"Two nights in a row, starting\")\n",
    "for gg in groups_of_two:\n",
    "    for nn in no_overlap:\n",
    "        if nn[0] == gg[0]:\n",
    "            print(nn[2])\n",
    "print(\"Three nights in a row, starting\")\n",
    "for gg in groups_of_three:\n",
    "    for nn in no_overlap:\n",
    "        if nn[0] == gg[0]:\n",
    "            print(nn[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db84be",
   "metadata": {},
   "source": [
    "### Which planets are on those chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "234fd004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "objects_in_clumps = []\n",
    "for gg in groups_of_two:\n",
    "    for dd in gg:\n",
    "        for nn in no_overlap:\n",
    "            if nn[0] == dd:\n",
    "                for obj in nn[3]:\n",
    "                    objects_in_clumps.append(obj)\n",
    "print(set(objects_in_clumps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d190e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_available_planets = list(set(planet_name))\n",
    "all_in_clumps = list(set(objects_in_clumps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2cb8d71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(all_available_planets))\n",
    "print(len(all_in_clumps))\n",
    "only_single_transits_possible=[]\n",
    "for pp in all_available_planets:\n",
    "    if pp not in all_in_clumps:\n",
    "        only_single_transits_possible.append(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977b08c",
   "metadata": {},
   "source": [
    "#### Find the missing singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3d896394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qatar-10 b\n",
      "19 2459690.75475 2459690.953583333\n",
      "20 2459695.690861111 2459695.8896944444\n",
      "Qatar-1 b\n",
      "6 2459691.789016667 2459691.94015\n",
      "7 2459701.7292944444 2459701.880427778\n",
      "8 2459708.8292944445 2459708.980427778\n",
      "14 2459711.666090278 2459711.8186319442\n",
      "KPS-1 b\n",
      "9 2459683.8143055555 2459683.967638889\n",
      "10 2459690.64 2459690.7933333335\n",
      "11 2459695.75875 2459695.9120833334\n",
      "12 2459707.7025 2459707.8558333335\n",
      "13 2459712.82125 2459712.9745833334\n",
      "WASP-103 b\n",
      "21 2459687.794590278 2459687.9859652775\n",
      "22 2459700.752229167 2459700.9436041666\n",
      "23 2459712.7841736116 2459712.975548611\n",
      "WASP-43 b\n",
      "28 2459685.6591833336 2459685.7908166666\n",
      "TrES-3 b\n",
      "15 2459688.802118056 2459688.942326389\n",
      "16 2459692.7208680557 2459692.8610763885\n",
      "17 2459705.7826736113 2459705.922881944\n",
      "18 2459709.7014236115 2459709.8416319443\n",
      "WASP-3 b\n",
      "24 2459702.7249444444 2459702.9222777775\n",
      "25 2459689.795888889 2459689.9902222217\n",
      "WASP-104 b\n",
      "26 2459688.656345139 2459688.8130993056\n",
      "27 2459695.6778729167 2459695.8346270835\n",
      "HAT-P-36 b\n",
      "0 2459691.814600695 2459691.990954861\n",
      "1 2459695.796545139 2459695.972899305\n",
      "2 2459699.7784895836 2459699.9548437498\n",
      "3 2459703.760434028 2459703.9367881943\n",
      "4 2459707.7423784724 2459707.9187326385\n",
      "5 2459711.724322917 2459711.900677083\n"
     ]
    }
   ],
   "source": [
    "for pp in only_single_transits_possible:\n",
    "    print(pp)\n",
    "    for ii in range(len(planet_name)):\n",
    "        if pp == planet_name[ii]:\n",
    "            print(ii,padded_start_jd[ii],padded_end_jd[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3e32e",
   "metadata": {},
   "source": [
    "### Output the full daily transit hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aa60a258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WASP-43 b', 2459685.6591833336, 2459685.7908166666]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_transit_hash[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c3bbb85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challis-transits-Apr-May-2022/Dates_for_all_transits.txt\n"
     ]
    }
   ],
   "source": [
    "outfile = out_folder+\"Dates_for_all_transits.txt\"\n",
    "print(outfile)\n",
    "file=open(outfile, \"w\")\n",
    "for ii,nn in enumerate(date_start_utc):\n",
    "    print(\"Day\",ii,date_start_utc[ii],file=open(outfile, \"a\"))\n",
    "    for hh in daily_transit_hash[ii]:\n",
    "        print(hh,file=open(outfile, \"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0b71e",
   "metadata": {},
   "source": [
    "### Output transit clump days for proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d81bda1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_days_in_clumps = sorted(list(set([item for sublist in groups_of_two for item in sublist])))\n",
    "all_days_in_clumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9971c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gg in all_days_in_clumps:\n",
    "    print(gg)\n",
    "    print(daily_transit_hash[gg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aa73ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List all no-overlap two transit nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "642f739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(no_overlap_day_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "20fffa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challis-transits-Apr-May-2022/Dates_with_two_plus_transits.txt\n"
     ]
    }
   ],
   "source": [
    "just_did = []\n",
    "outfile = out_folder+\"Dates_with_two_plus_transits.txt\"\n",
    "file=open(outfile, \"w\")\n",
    "for ii,nn in enumerate(no_overlap_day_nums):\n",
    "    if nn not in just_did:\n",
    "        print(\"Day\",nn,no_overlap_dates[ii],file=open(outfile, \"a\"))\n",
    "        for hh in daily_transit_hash[nn]:\n",
    "            print(hh,file=open(outfile, \"a\"))\n",
    "    # stop duplicates for nights with 3+ transits\n",
    "    just_did.append(nn)\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adc9d2",
   "metadata": {},
   "source": [
    "## Output fuller target list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b5853cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_hash = {}; dec_hash={}; vmag_hash={}\n",
    "for ii,pp in enumerate(planet_name):\n",
    "    if pp not in ra_hash.keys():\n",
    "        pos = planet_name.index(pp)\n",
    "        ra_hash[pp] = planet_ra[pos]\n",
    "        dec_hash[pp] = planet_dec[pos]\n",
    "        vmag_hash[pp] = planet_vmag[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c66a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_cal_Time = Time(padded_start_jd, format='jd')\n",
    "in_cal = in_cal_Time.utc\n",
    "in_cal.format = 'isot'\n",
    "eg_cal_Time = Time(padded_end_jd, format='jd')\n",
    "eg_cal = eg_cal_Time.utc\n",
    "eg_cal.format = 'isot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4d7571f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challis-transits-Apr-May-2022/Full_transits_Challis-transits-Apr-May-2022.txt\n"
     ]
    }
   ],
   "source": [
    "outfile = out_folder+\"Full_transits_\"+when+\".txt\"\n",
    "file=open(outfile, \"w\")\n",
    "print(\"\\t\".join([\"Planet\", \"RA\", \"Dec\", \"V\", \"Depth\", \"Padded_Start_UT_JD\", \"Padded_End_UT_JD\", \n",
    "      \"Padded_start_UT_cal\", \"Padded_end_UT_cal\"]), file=open(outfile, \"a\"))\n",
    "for ii,nn in enumerate(date_start_utc):\n",
    "    print(\"Day\",ii,date_start_utc[ii],\":\",file=open(outfile, \"a\"))\n",
    "    for hh in daily_transit_hash[ii]:\n",
    "        pp = hh[0]\n",
    "        in_cal_Time = Time(hh[1], format='jd')\n",
    "        in_cal = in_cal_Time.utc\n",
    "        in_cal.format = 'isot'\n",
    "        eg_cal_Time = Time(hh[2], format='jd')\n",
    "        eg_cal = eg_cal_Time.utc\n",
    "        eg_cal.format = 'isot'\n",
    "        print(\"\\t\".join([pp, ra_hash[pp], dec_hash[pp], str(vmag_hash[pp]), str(tar_depth_hash[pp]),\n",
    "              str(hh[1]), str(hh[2]), str(in_cal), str(eg_cal)]),\n",
    "              file=open(outfile, \"a\"))\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e5928cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WASP-103 b',\n",
       " '16h37m15.57s',\n",
       " '07d11m00.15s',\n",
       " '12.402',\n",
       " '1.2',\n",
       " '2459712.7841736116',\n",
       " '2459712.975548611',\n",
       " <Time object: scale='utc' format='isot' value=2022-05-13T06:49:12.600>,\n",
       " <Time object: scale='utc' format='isot' value=2022-05-13T11:24:47.400>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp, ra_hash[pp], dec_hash[pp], str(vmag_hash[pp]), str(tar_depth_hash[pp]), str(hh[1]), str(hh[2]), in_cal, eg_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b15d3a",
   "metadata": {},
   "source": [
    "# Hi I am a note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848841fc",
   "metadata": {},
   "source": [
    "This is the end of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f81eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
